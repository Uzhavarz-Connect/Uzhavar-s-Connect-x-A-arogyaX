{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary dependencies\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "from langchain.memory import ChatMessageHistory \n",
    "from langchain_core.messages import AIMessage,HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "#loading the environmental variables set up in the env file \n",
    "load_dotenv()\n",
    "\n",
    "#initialising the LLM for the Chatbot which we are going to integerate into the website\n",
    "llm=ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "\n",
    "#creating a prompt template that describes the ai what to do \n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"you are a helpful assistant that guides the user about the website that we will integerate with you.\" \n",
    "        ),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\",\"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#initialising String output Parser such that the output is clear to the user\n",
    "parser=StrOutputParser()\n",
    "\n",
    "#chaining our model with a prompt template so that our llm model can know with respect to what template the answers can be generated and the output can be structured \n",
    "chain=prompt|llm|parser\n",
    "\n",
    "#memory for the chatbot to store both human and the LLM model's response\n",
    "chat_history=ChatMessageHistory()\n",
    "\n",
    "#setting the flag value\n",
    "flag=True\n",
    "while(flag):\n",
    "    #input from the user as the query\n",
    "    query=input(\"How Can I Help You:\")\n",
    "\n",
    "    #adding user messages to \n",
    "    chat_history.add_user_message(query)\n",
    "\n",
    "    # Invoke the model to answer the user's question\n",
    "    try:\n",
    "        response = chain.invoke(\n",
    "            {\n",
    "               \"user_input\":query,\n",
    "                \"chat_history\":chat_history.messages, \n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Save the model's response into the memory\n",
    "        #main_response = response.content\n",
    "        chat_history.add_ai_message(response)\n",
    "\n",
    "        # Print the response\n",
    "        print(response)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error invoking model: {e}\")\n",
    "\n",
    "    # Ask the user whether to continue or not\n",
    "    finally:\n",
    "        answer = input(\"Do you want to continue? (yes/no): \")\n",
    "        if answer.lower() == \"no\":\n",
    "            flag = False\n",
    "\n",
    "\n",
    "##clearing the chat history            \n",
    "#chat_history.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User said: what is transformers in AI?\n",
      "AI responded: Transformers are a type of neural network architecture that has revolutionized the field of artificial intelligence, particularly in natural language processing (NLP).  They were introduced in the 2017 paper \"Attention is All You Need\" by Vaswani et al. and have since become the dominant model for many NLP tasks.\n",
      "\n",
      "Here's a breakdown of what makes transformers important:\n",
      "\n",
      "* **Attention Mechanism:**  The core innovation of transformers is the **self-attention** mechanism.  Traditional recurrent neural networks (RNNs) process sequential data step-by-step, which can be slow and struggle with long-range dependencies.  Self-attention allows the model to consider all words in a sequence simultaneously when processing each word, weighing the importance of each word in relation to the others. This parallel processing makes transformers significantly faster than RNNs and allows them to capture relationships between distant words more effectively.\n",
      "\n",
      "* **No Recurrence or Convolution:** Unlike RNNs or convolutional neural networks (CNNs), transformers don't rely on sequential processing or local receptive fields. This makes them more flexible and adaptable to different types of data.\n",
      "\n",
      "* **Scalability:** Transformers can be scaled up to very large sizes, enabling them to learn from massive datasets and achieve state-of-the-art performance on various tasks.  This scalability has led to the development of large language models (LLMs) like GPT-3, BERT, and others.\n",
      "\n",
      "* **Transfer Learning:** Transformers can be pre-trained on a large corpus of text and then fine-tuned for specific tasks with relatively small amounts of data.  This transfer learning approach has greatly improved the efficiency of NLP model development.\n",
      "\n",
      "* **Parallelization:** The architecture of transformers allows for highly parallel processing, making them much faster to train than traditional sequential models.\n",
      "\n",
      "**Key Concepts within Transformers:**\n",
      "\n",
      "* **Encoder-Decoder Structure:** Many transformer models use an encoder-decoder structure. The encoder processes the input sequence and generates a contextualized representation. The decoder then uses this representation to generate an output sequence, such as a translation or a summary.\n",
      "\n",
      "* **Multi-Head Attention:** Instead of using a single attention mechanism, transformers often employ multi-head attention, which allows the model to attend to different parts of the input sequence in different ways.\n",
      "\n",
      "* **Positional Encoding:** Since transformers don't inherently process sequences sequentially, positional encodings are added to the input embeddings to provide information about the order of words.\n",
      "\n",
      "**Impact and Applications:**\n",
      "\n",
      "Transformers have had a profound impact on NLP and are used in a wide range of applications, including:\n",
      "\n",
      "* **Machine Translation:**  Significantly improved the accuracy and fluency of machine translation systems.\n",
      "* **Text Summarization:**  Generating concise and informative summaries of longer texts.\n",
      "* **Question Answering:**  Providing accurate answers to questions based on given text.\n",
      "* **Text Generation:**  Creating human-like text for various purposes, including creative writing and chatbots.\n",
      "* **Sentiment Analysis:**  Determining the emotional tone of a piece of text.\n",
      "* **Code Generation:**  Generating code in various programming languages.\n",
      "\n",
      "In short, transformers are a powerful and versatile neural network architecture that has revolutionized the field of NLP and continues to drive advancements in AI.\n"
     ]
    }
   ],
   "source": [
    "#to view chat history \n",
    "# Iterate over the messages in the chat history\n",
    "for message in chat_history.messages:\n",
    "    if isinstance(message, HumanMessage):\n",
    "        print(f\"User said: {message.content}\")\n",
    "    elif isinstance(message, AIMessage):\n",
    "        print(f\"AI responded: {message.content}\")\n",
    "\n",
    "\n",
    "#to clear the chat history \n",
    "#list(chat_history)\n",
    "#chat_history.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mistralai in c:\\users\\rexjo\\anaconda3\\envs\\newenv\\lib\\site-packages (0.0.8)\n",
      "Collecting mistralai\n",
      "  Downloading mistralai-1.6.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting eval-type-backport>=0.2.0 (from mistralai)\n",
      "  Using cached eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: httpx>=0.28.1 in c:\\users\\rexjo\\anaconda3\\envs\\newenv\\lib\\site-packages (from mistralai) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.10.3 in c:\\users\\rexjo\\anaconda3\\envs\\newenv\\lib\\site-packages (from mistralai) (2.10.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rexjo\\anaconda3\\envs\\newenv\\lib\\site-packages (from mistralai) (2.9.0.post0)\n",
      "Collecting typing-inspection>=0.4.0 (from mistralai)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\rexjo\\anaconda3\\envs\\newenv\\lib\\site-packages (from httpx>=0.28.1->mistralai) (3.7.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\rexjo\\anaconda3\\envs\\newenv\\lib\\site-packages (from httpx>=0.28.1->mistralai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\rexjo\\anaconda3\\envs\\newenv\\lib\\site-packages (from httpx>=0.28.1->mistralai) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\rexjo\\anaconda3\\envs\\newenv\\lib\\site-packages (from httpx>=0.28.1->mistralai) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\rexjo\\anaconda3\\envs\\newenv\\lib\\site-packages (from httpcore==1.*->httpx>=0.28.1->mistralai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\rexjo\\anaconda3\\envs\\newenv\\lib\\site-packages (from pydantic>=2.10.3->mistralai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\rexjo\\anaconda3\\envs\\newenv\\lib\\site-packages (from pydantic>=2.10.3->mistralai) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\rexjo\\anaconda3\\envs\\newenv\\lib\\site-packages (from pydantic>=2.10.3->mistralai) (4.13.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rexjo\\anaconda3\\envs\\newenv\\lib\\site-packages (from python-dateutil>=2.8.2->mistralai) (1.17.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\rexjo\\anaconda3\\envs\\newenv\\lib\\site-packages (from anyio->httpx>=0.28.1->mistralai) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\rexjo\\anaconda3\\envs\\newenv\\lib\\site-packages (from anyio->httpx>=0.28.1->mistralai) (1.2.2)\n",
      "Downloading mistralai-1.6.0-py3-none-any.whl (288 kB)\n",
      "Using cached eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
      "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-inspection, eval-type-backport, mistralai\n",
      "  Attempting uninstall: mistralai\n",
      "    Found existing installation: mistralai 0.0.8\n",
      "    Uninstalling mistralai-0.0.8:\n",
      "      Successfully uninstalled mistralai-0.0.8\n",
      "Successfully installed eval-type-backport-0.2.2 mistralai-0.4.2 typing-inspection-0.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-mistralai 0.0.2.post1 requires langchain-core<0.2,>=0.1, but you have langchain-core 0.3.48 which is incompatible.\n",
      "langchain-mistralai 0.0.2.post1 requires mistralai<0.0.9,>=0.0.8, but you have mistralai 1.6.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade mistralai"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
